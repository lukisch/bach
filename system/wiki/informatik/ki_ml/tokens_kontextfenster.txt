# Portabilitaet: UNIVERSAL
# Zuletzt validiert: 2026-02-15 (Claude)
# Naechste Pruefung: 2026-08-15
# Quellen: [https://platform.openai.com/docs/guides/tokens], [https://www.anthropic.com/news/claude-3-family], [https://arxiv.org/pdf/2310.16944.pdf]

TOKENS UND KONTEXTFENSTER IN LLMs
==================================

Stand: 2026-02-15

Erklaerung der fundamentalen Konzepte, die das Verhalten und die Leistung von 
Large Language Models (wie Claude, GPT-4, Gemini) bestimmen.

WAS SIND TOKENS?
----------------
Tokens sind die kleinsten semantischen Einheiten, die ein Language Model verarbeitet.
Sie sind NICHT 1:1 Zeichen. Ein Token repraesentiert typischerweise:
  - Ein Wort (z.B. "Katze")
  - Ein Wortfragment (z.B. "ing" in "walking")
  - Ein Sonderzeichen (z.B. ".", "!", "?")
  - Leerraum und Zeilenumbrueche

Tokenisierung erfolgt durch einen Tokenizer, einen trainierten Algorithmus 
(meist Byte-Pair-Encoding / BPE).

BEISPIEL - TOKENISIERUNG:
  Text:        "Claude ist ein KI-Modell."
  Tokens:      ["Claude", " ist", " ein", " KI", "-", "Modell", "."]
  Token-Count: 7
  
WICHTIG: Deutsche Texte haben mehr Tokens als englische, weil:
  - Lange zusammengesetzte Woerter (z.B. "Donaudampfschifffahrtsgesellschaftskapitaen")
  - Umlaute werden oft mehrere Token
  
Beispiel: "Geschäftsbericht" = 2-3 Tokens (Deutsch) vs "Business Report" = 2 Tokens (Englisch)

WAS IST EIN KONTEXTFENSTER?
---------------------------
Das Kontextfenster ist die MAXIMALE Anzahl von Tokens, die ein Modell 
auf einmal verarbeiten kann. Es ist wie ein "Aufmerksamkeitsfenster":

  - Tokens IN = User-Input + System-Prompt + bisherige Konversation
  - Tokens OUT = Model-Response

Das Kontextfenster bestimmt:
  1. Wie viel Konversationshistorie kann erinnert werden?
  2. Wie grosse Dateien koennen auf einmal verarbeitet werden?
  3. Wie viel Kontext-Wissen kann uebergeben werden?

BEISPIEL - KONTEXTBUDGET:
  Modell: Claude 3 Opus
  Kontextfenster: 200.000 Tokens
  
  Budget-Verteilung:
    System-Prompt:           1.000 Tokens
    Bisherige Konversation:  80.000 Tokens
    Aktuelle User-Input:     50.000 Tokens
    Available fuer Response: 69.000 Tokens
  
  Das Modell kann maximal 200.000 gesamt nutzen, nicht mehr.

KONTEXTFENSTER BEI VERSCHIEDENEN MODELLEN:
-------------------------------------------
  Modell                    Kontextfenster    Kosten      Anwendung
  ─────────────────────────────────────────────────────────────
  Claude 3 Haiku            200.000          Billig      Schnell, viel Text
  Claude 3.5 Sonnet         200.000          Mittel      Allrounder
  Claude 3 Opus             200.000          Teuer       Komplex, präzise
  GPT-4o                    128.000          Teuer       Text + Bilder
  Gemini 2.0                1.000.000        Moderat     Extrem lange Kontexte
  Llama 2 (Open Source)      4.096-8.192     Lokal       Lokale Nutzung

PRAKTISCHE IMPLIKATIONEN IN BACH
---------------------------------
1. DOKUMENTEN-VERARBEITUNG
   BACH kann ganze Handbuecher uebergeben (< 200k Tokens):
   - Gesamtes FEATURES.md (42k Zeichen ≈ 10k Tokens) passt locker
   - Gesamter Codebase als Kontext (bei Bedarf)
   
2. GESPRAECH-SPEICHER
   Bei langen Konversationen:
   - Alte Messages werden automatisch gekuerzt/zusammengefasst
   - Nach ~50-100 User-Messages sollte Context-Cleanup erfolgen
   
3. TOKEN-KOSTEN
   Kosten = (Input-Tokens × Input-Preis) + (Output-Tokens × Output-Preis)
   Beispiel (Claude 3 Opus):
     Input:  100.000 Tokens × $0.015/1K = $1.50
     Output:  10.000 Tokens × $0.060/1K = $0.60
     Gesamt: $2.10 pro Aufruf

4. KONTEXTFENSTER-STRATEGIEN
   a) Lazy Loading: Nur relevante Teile laden
   b) Summarization: Alte Konversation zusammenfassen
   c) Vector Search: Relevante Chunks abrufen statt gesamtem Kontext
   d) Multi-Agent: Aufgaben auf mehrere Aufrufe verteilen

TOKEN-ZAEHLUNG IN DER PRAXIS
----------------------------
Approximativer Faktor:
  1 Token ≈ 4 Zeichen (englisch)
  1 Token ≈ 3 Zeichen (deutsch - weniger effizient)

Schnelle Schätzung:
  Wortanzahl / 0.75 ≈ Token-Anzahl (englisch)
  Wortanzahl / 0.5 ≈ Token-Anzahl (deutsch)

Beispiel BACH ARCHITECTURE.md:
  Groesse:        42.724 Zeichen
  Geschaetzte Tokens:  ~10.700 (deutsch-optimiert)

KONTEXTFENSTER-PLANUNG
----------------------
Faustregel fuer effiziente Agent-Nutzung:
  
  Reserve fuer Response:  20-30% des Fensters
  Nutzbar fuer Kontext:   70-80% des Fensters
  
  Bei Claude mit 200k:
    Verfügbar:  140.000 Tokens
    System:        1.000 Tokens
    Geschichte:   50.000 Tokens
    Aktueller Input: Bis zu 89.000 Tokens

SIEHE AUCH
----------
wiki/informatik/ki_ml/biomimetisches_lernen.txt     Lernen in KI-Systemen
wiki/was_ist_bach.txt                               BACH-Architektur und KI-Integration
wiki/ki_preise.txt                                  Kostenvergleich verschiedener Modelle
