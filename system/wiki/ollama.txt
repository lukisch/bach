OLLAMA - LOKALER LLM-SERVER & CLOUD
====================================

Stand: 2026-01-24

WAS IST OLLAMA?
---------------
Ollama ist ein Open-Source-Tool zum lokalen Ausfuehren von
Large Language Models (LLMs) auf dem eigenen Computer.
Es ermoeglicht KI-Inferenz ohne Cloud-Anbindung.

Seit 2025 bietet Ollama auch CLOUD-Modelle fuer groessere
Modelle, die lokal nicht laufen.

INSTALLATION
------------
  Pfad:     C:\Users\User\AppData\Local\Programs\Ollama\
  Config:   C:\Users\User\.ollama\
  Version:  0.13.3 (Stand: 2026-01-22)

INSTALLIERTE MODELLE (LOKAL)
----------------------------
  Mistral 7B          4.4 GB    Chat/Completion (Q4_K_M)
  nomic-embed-text    274 MB    Embeddings (F16)

API-ENDPUNKT
------------
  URL:      http:/127.0.0.1:11434
  Status:   Aktiv (startet automatisch bei Bedarf)

API-BEISPIELE
-------------
  # Modelle auflisten
  curl http:/127.0.0.1:11434/api/tags

  # Text generieren
  curl http:/127.0.0.1:11434/api/generate -d '{
    "model": "Mistral",
    "prompt": "Deine Frage",
    "stream": false
  }'

  # Embeddings erzeugen
  curl http:/127.0.0.1:11434/api/embeddings -d '{
    "model": "nomic-embed-text",
    "prompt": "Text zum Einbetten"
  }'

PYTHON-INTEGRATION
------------------
  import requests

  def ask_ollama(prompt, model="Mistral"):
      response = requests.post(
          "http:/127.0.0.1:11434/api/generate",
          json={"model": model, "prompt": prompt, "stream": False}
      )
      return response.json()["response"]

CLI-BEFEHLE
-----------
  ollama list           Installierte Modelle anzeigen
  ollama run Mistral    Interaktiver Chat
  ollama pull llama3    Neues Modell herunterladen
  ollama rm MODEL       Modell loeschen

BACH-INTEGRATION (GEPLANT)
--------------------------
  - bach ollama ask "Frage"       Schnelle Anfrage
  - bach ollama embed "Text"      Embedding generieren
  - Daemon-Jobs fuer automatische Aufgaben
  - Offline-Fallback bei Netzwerkausfall

VORTEILE
--------
  + Keine API-Kosten
  + Datenschutz (lokal)
  + Unbegrenzte Anfragen
  + Funktioniert offline

NACHTEILE
---------
  - Benoetigt GPU fuer gute Performance
  - Modelle kleiner als Cloud-LLMs
  - Braucht ~8GB RAM fuer Mistral

SPEICHERORT DER MODELLE
-----------------------
  C:\Users\User\.ollama\models\blobs\    (4.6 GB gesamt)
  C:\Users\User\.ollama\models\manifests\

OLLAMA CLOUD (NEU)
==================

QUELLEN
-------
  Web-Recherche durchgefuehrt am 2026-01-24:
  - ollama.com/cloud
  - ollama.com/pricing
  - docs.ollama.com/cloud

WAS IST OLLAMA CLOUD?
---------------------
Ollama Cloud ermoeglicht das Ausfuehren von Modellen auf
leistungsstarken Cloud-GPUs. Viele neue Modelle sind zu gross
fuer lokale GPUs oder laufen lokal zu langsam.

Ollama Cloud bietet:
  - Schnelle Inferenz auf Cloud-GPUs
  - Zugang zu grossen Modellen (z.B. Gemini 3 Pro Preview)
  - Nutzbar via Ollama App, CLI und API

PRICING-PLAENE
--------------
  Free
    - Leichte Nutzung: Chat, schnelle Fragen, Modelle testen
    - Stunden- und Wochenlimits

  Pro
    - Tagesarbeit: RAG, Dokumentenanalyse, Coding-Tasks
    - Hoehere Limits und Concurrency
    - Ideal fuer regulaere Arbeit

  Max
    - Heavy/Sustained Usage
    - Coding-Agents, Batch-Processing, Daten-Automatisierung
    - Hoechste Limits und Concurrency

  (Konkrete Preise auf ollama.com/pricing)

PREMIUM MODEL REQUESTS
----------------------
Zusaetzliche Requests fuer groessere Modelle wie Gemini 3 Pro Preview.
Diese zaehlen NICHT gegen Stunden-/Wochenlimits und sind in
begrenzter Menge pro Monat verfuegbar.

CONCURRENCY-LIMITS
------------------
Cloud-Modelle haben Concurrency-Limits pro Plan, um dedizierte
Kapazitaet fuer Multi-Model-Tasks zu gewaehrleisten:
  - Coding-Agents
  - Deep Research
  - Batch Data Processing

WICHTIG: Lokal laufende Modelle sind IMMER unlimitiert!

API-ZUGANG (CLOUD)
------------------
Cloud-Modelle sind direkt ueber ollama.com API verfuegbar.
ollama.com agiert als Remote Ollama Host.

  1. API-Key erstellen auf ollama.com
  2. Umgebungsvariable setzen:
     set OLLAMA_API_KEY=dein-api-key

  Auch kompatibel mit OpenAI-kompatiblem API-Endpoint!

DATENSCHUTZ
-----------
  - Alle Cloud-Requests verschluesselt (in transit)
  - Keine Speicherung von Prompts oder Outputs
  - Ollama laeuft lokal weiterhin voellig offline

LOKAL vs CLOUD
--------------
  Lokal:
    + Keine Kosten
    + Datenschutz (voellig offline)
    + Unbegrenzte Anfragen
    - Begrenzt durch lokale Hardware
    - Grosse Modelle laufen langsam/nicht

  Cloud:
    + Zugang zu grossen Modellen
    + Schnelle Inferenz
    + Keine Hardware-Anforderungen
    - Kosten (je nach Plan)
    - Limits (Stunden/Wochen)

NUETZLICHE LINKS
----------------
  Ollama Cloud:     https:/ollama.com/cloud
  Pricing:          https:/ollama.com/pricing
  Cloud Doku:       https:/docs.ollama.com/cloud
  Cloud Modelle:    https:/ollama.com/search?c=cloud

SIEHE AUCH
----------
  wiki/gemini.txt         Google Gemini KI
  wiki/antigravity.txt    Google Antigravity IDE
  wiki/chatgpt.txt        OpenAI ChatGPT
  wiki/copilot.txt        Microsoft Copilot
  docs/docs/docs/help/partners.txt            KI-Partner im BACH-System
