AI-PORTABLE - LOKALE KI-TOOLSAMMLUNG
=====================================

WAS IST AI-PORTABLE?
--------------------
AI-Portable ist eine portable Sammlung von KI-Tools fuer
lokale Nutzung ohne Cloud-Abhaengigkeit. Es kombiniert:
  - RAG-Pipeline (Retrieval Augmented Generation)
  - Lokale LLMs via Ollama
  - Vektordatenbanken (ChromaDB, LanceDB)
  - Spezialisierte Modi (ICF, Coding, RPG)

SPEICHERORT
-----------
  C:\Users\User\OneDrive\KI&AI\AI-Portable\

VERZEICHNISSTRUKTUR
-------------------
  AI-Portable/
  ├── models/
  │   ├── llm/           Sprachmodelle (Mistral, Llama)
  │   ├── embeddings/    Embedding-Modelle (nomic-embed-text)
  │   └── tts/           Text-to-Speech (optional)
  │
  ├── db/
  │   ├── chroma/        Vektordatenbank (primaer)
  │   ├── lancedb/       Alternative Vektordatenbank
  │   └── sqlite/        Metadaten-Speicher
  │
  ├── documents/
  │   ├── icf/           ICF-Aufzeichnungen
  │   ├── foerderung/    Foerderplan-Material
  │   ├── rpg/           Rollenspiel-Welten
  │   └── code/          Code-Snippets
  │
  ├── rag/
  │   ├── pipeline.py    RAG-Hauptpipeline
  │   ├── ingest.py      Dokumente indexieren
  │   └── query.py       Abfragen ausfuehren
  │
  ├── prompts/
  │   ├── icf.txt        ICF-Assistent Prompt
  │   ├── coding.txt     Code-Assistent Prompt
  │   ├── rpg.txt        RPG-Spielleiter Prompt
  │   └── foerderung.txt Foerderplan-Prompt
  │
  ├── scripts/
  │   ├── fill_icf_template.py  ICF-Berichte generieren
  │   ├── coding_mode.py        Code-Assistent
  │   ├── rpg_mode.py           RPG-Modus
  │   ├── dashboard.py          Uebersichts-GUI
  │   └── extract_pdfs.py       PDF-Extraktion
  │
  ├── templates/
  │   └── icf_template.docx     ICF-Bericht-Vorlage
  │
  └── venv/              Portable Python-Umgebung

RAG-PIPELINE
------------
Die RAG-Pipeline nutzt ChromaDB und Ollama:

  1. Dokumente indexieren (ingest.py)
     - Liest .txt und .md Dateien
     - Erzeugt Embeddings via Ollama
     - Speichert in ChromaDB

  2. Abfragen (query.py, pipeline.py)
     - Embedding der Frage
     - Aehnlichkeitssuche in ChromaDB
     - Kontext an LLM uebergeben
     - Antwort generieren

VERWENDETE MODELLE
------------------
  LLM:        Mistral:instruct (via Ollama)
  Embeddings: nomic-embed-text (via Ollama)

MODI / SPEZIALISIERUNGEN
------------------------

  ICF-MODUS (fill_icf_template.py)
  --------------------------------
  Erstellt ICF-Berichte (International Classification of
  Functioning) fuer Autismus-Dokumentation:
    - Koerperfunktionen (b)
    - Aktivitaeten & Partizipation (d)
    - Umweltfaktoren (e)
    - Koerperstrukturen (s)

  CODING-MODUS (coding_mode.py)
  -----------------------------
  Code-Assistent mit Fokus auf:
    - Sauberen, lesbaren Code
    - Erklaerungen und Alternativen
    - Refactoring-Vorschlaege

  RPG-MODUS (rpg_mode.py)
  -----------------------
  Pen-and-Paper Spielleiter:
    - Immersive Weltenbeschreibung
    - NPC-Dialoge
    - Entscheidungsoptionen

INSTALLATION / SETUP
--------------------
  # Windows
  powershell scripts\setup_portable.ps1

  # Linux/Mac
  bash scripts/setup_portable.sh

  # Manuell
  python -m venv venv
  venv\Scripts\activate
  pip install chromadb lancedb ollama-client

ABHAENGIGKEITEN
---------------
  - Python 3.10+
  - Ollama (muss laufen!)
  - chromadb
  - lancedb (optional)
  - python-docx (fuer ICF-Templates)
  - ollama-client

BACH-INTEGRATION (POTENZIAL)
----------------------------
  1. RAG-Pipeline fuer BACH-Wissensbasis
  2. ICF-Modus fuer PsycoTools
  3. Embedding-Suche fuer Help-System
  4. Offline-Fallback fuer LLM-Abfragen

SIEHE AUCH
----------
  wiki/ollama.txt       Ollama LLM-Server (Voraussetzung!)
  wiki/gemini.txt       Google Gemini Alternative
  C:\Users\User\OneDrive\Software Entwicklung\PsycoTools\
