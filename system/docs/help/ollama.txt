# Portabilitaet: UNIVERSAL
# Version: 1.0.1
# Zuletzt validiert: 2026-02-08 (Forensik - URL/Import/Syntax-Fixes)
# Naechste Pruefung: 2026-08-08

BACH OLLAMA-INTEGRATION
=======================
Lokaler LLM-Server fuer Token-Sparmodus und Offline-Nutzung.

BEFEHLE
-------
bach ollama status           Verbindung pruefen, installierte Modelle
bach ollama ask "prompt"     Direkte Anfrage an Ollama
bach ollama embed "text"     Embedding generieren
bach ollama models           Verfuegbare Modelle auflisten

OPTIONEN
--------
--model=NAME              Modell waehlen (default: llama3.2)

BEISPIELE
---------
# Status pruefen
bach ollama status

# Einfache Frage
bach ollama ask "Was ist BACH?" --model=llama3.2

# Code-Review delegieren
bach ollama ask "Review diesen Code: def add(a,b): return a+b"

# Embedding fuer Suche
bach ollama embed "Suchtext fuer RAG"

INTEGRATION MIT PARTNER-SYSTEM
------------------------------
Ollama ist als lokaler Partner registriert und wird automatisch
bei hohem Token-Verbrauch bevorzugt:

Zone 3 (60-80% Tokens):  Ollama wird bevorzugt
Zone 4 (80-100%):        Nur Human/Notfall (Ollama noch moeglich)

Automatische Delegation:
  bach partner delegate "Task" --to=ollama

Fallback bei Netzwerkproblemen:
  bach ollama ask "Task"   (direkte lokale Ausfuehrung)

VORAUSSETZUNGEN
---------------
- Ollama muss lokal installiert sein
- Standard-Port: http://localhost:11434
- Mindestens ein Modell muss gepullt sein

Installation pruefen:
  ollama --version
  ollama list

Modell installieren:
  ollama pull llama3.2
  ollama pull codellama

TROUBLESHOOTING
---------------
Fehler: "Connection refused"
  → Ollama-Server starten: ollama serve
  → Port pruefen: http://localhost:11434

Fehler: "Model not found"
  → Modell pullen: ollama pull llama3.2
  → Modellnamen pruefen: ollama list

Langsame Antworten:
  → Kleineres Modell waehlen (llama3.2 statt llama3.1:70b)
  → GPU-Nutzung pruefen (CUDA/Metal)

Speicherprobleme:
  → Kleineres Modell verwenden
  → Andere Modelle entladen: ollama stop

KONFIGURATION
-------------
Ollama-Einstellungen in BACH:
  - Partner-ID: ollama
  - Typ: local
  - Standardmodell: llama3.2 (konfigurierbar)
  - Timeout: 120 Sekunden

Anpassung via:
  bach partner info ollama

SIEHE AUCH
----------
bach help partner          Partner-System Uebersicht
bach help delegate         Task-Delegation
bach help tools            Tool-Inventar (ollama_client.py)
